{"cells":[{"cell_type":"code","source":["import time\n","import base64\n","import json\n","from pathlib import Path\n","\n","import requests"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"972ae617-68eb-4c19-ba7a-c8ef365c30a3"},{"cell_type":"code","source":["workspace_id = notebookutils.runtime.context[\"currentWorkspaceId\"]\n","folder_name = \"acr\"\n","lakehouse_name = \"LH_acr\"\n","\n","headers = {\"Authorization\": f\"Bearer {notebookutils.credentials.getToken('pbi')}\"}"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"730138d4-5403-422b-887e-395d5625de5f"},{"cell_type":"markdown","source":["## Get artifacts from Github repo\n","https://github.com/Guust-Franssens/acr\n"],"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"nteract":{"transient":{"deleting":false}}},"id":"d51d7e6d-12ca-4ce2-b74a-93dd0a4319ee"},{"cell_type":"code","source":["url = \"https://api.github.com/repos/guust-franssens/acr/contents/fabric\"\n","response = requests.get(url)\n","response.raise_for_status()\n","gh_items = response.json()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"17dfe772-0c3c-4dcf-b6c3-af5d18bbb59a"},{"cell_type":"markdown","source":["## Deploy the items into a folder"],"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"nteract":{"transient":{"deleting":false}}},"id":"c43ab25f-6b1b-41d6-be49-18ff6c61057a"},{"cell_type":"code","source":["# create the folder\n","response = requests.get(f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/folders\", headers=headers)\n","response.raise_for_status()\n","if folder_name in [folder[\"displayName\"] for folder in response.json()[\"value\"]]:\n","    folder_id = next(folder[\"id\"] for folder in response.json()[\"value\"] if folder[\"displayName\"] == folder_name)\n","else:\n","    url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/folders\"\n","    response = requests.post(url, json={\"displayName\": folder_name}, headers=headers)\n","    response.raise_for_status()\n","    folder_id = response.json()[\"id\"]\n","\n","print(folder_id)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"27ea62ab-ab7c-498c-b3b2-b25fd8358653"},{"cell_type":"code","source":["def get_existing_items_in_folder(workspace_id, folder_id):\n","    url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/items?rootFolderId={folder_id}\"\n","    response = requests.get(url, headers=headers)\n","    response.raise_for_status()\n","    return response.json()[\"value\"]\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"ce92e159-04fa-482a-a7a6-4b0d92c3f3d2"},{"cell_type":"code","source":["def deploy_lakehouse(lakehouse_name, existing_items, folder_id=None):\n","    lakehouse = [item for item in existing_items if item[\"type\"] == \"Lakehouse\" and item[\"displayName\"] == lakehouse_name]\n","    if lakehouse:\n","        print(f\"⚠️ Lakehouse already exists under name {lakehouse_name}\")\n","        return lakehouse[0][\"id\"]\n","\n","    url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/lakehouses\"\n","    body = {\n","        \"displayName\": lakehouse_name,\n","        \"folderId\": folder_id\n","    }\n","    response = requests.post(url, headers=headers, json=body)\n","    response.raise_for_status()\n","    if response.status_code == 202:\n","        print(f\"✅ Lakehouse deployment initiated, requesting update after {response.headers['Retry-After']}\")\n","        time.sleep(int(response.headers['Retry-After']))\n","        response = requests.get(response.headers['Location'], headers=headers)\n","        response.raise_for_status()\n","\n","    return response.json()[\"id\"]"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"6162ce15-c2a8-4c86-870b-a2699395b213"},{"cell_type":"code","source":["def deploy_notebook(notebook_name, notebook_content, existing_items, folder_id=None):\n","    # Check if the notebook already exists in the folder\n","    existing_notebook = [item[\"id\"] for item in existing_items if item[\"type\"] == \"Notebook\" and item[\"displayName\"] == notebook_name]\n","\n","    body = {\n","        \"definition\": {\n","            \"format\": \"ipynb\",\n","            \"parts\": [\n","                {\n","                    \"path\": \"notebook-content.ipynb\",\n","                    \"payload\": base64.b64encode(notebook_content),\n","                    \"payloadType\": \"InlineBase64\"\n","                }\n","            ]\n","        }\n","    }\n","    if existing_notebook:\n","        notebook_id = existing_notebook[0]\n","        url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/notebooks/{notebook_id}/updateDefinition\"\n","        print(f\"⚠️ Notebook already exists, updating {notebook_name}...\")\n","    else:\n","        url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/notebooks\"\n","        body[\"displayName\"] = notebook_name\n","        if folder_id:\n","            body[\"folderId\"] = folder_id\n","        print(f\"✅ Creating {notebook_name}\")\n","\n","\n","    response = requests.post(url, headers=headers, json=body)\n","    response.raise_for_status()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"9199d0c0-20bd-40e7-a06d-2e3c5b3f9ef5"},{"cell_type":"code","source":["def deploy_pipeline(gh_pipeline, existing_items, folder_id=None):\n","    pipeline_name = Path(gh_pipeline[\"name\"]).stem\n","\n","    existing_pipeline = [item[\"id\"] for item in fabric_items if item[\"type\"] == \"DataPipeline\" and item[\"displayName\"] == pipeline_name]\n","\n","    response = requests.get(gh_pipeline[\"download_url\"])\n","    response.raise_for_status()\n","    pipeline_definition = response.json()\n","\n","    nb_100_id = next(item[\"id\"] for item in fabric_items if \"NB_100_\" in item[\"displayName\"])\n","    nb_200_id = next(item[\"id\"] for item in fabric_items if \"NB_200_\" in item[\"displayName\"])\n","    nb_300_id = next(item[\"id\"] for item in fabric_items if \"NB_300_\" in item[\"displayName\"])\n","\n","    pipeline_activities = pipeline_definition[\"definition\"][\"parts\"][0][\"payload\"][\"properties\"][\"activities\"]\n","    pipeline_activities[0][\"typeProperties\"] = {\"notebookId\": nb_100_id, \"workspaceId\": workspace_id}\n","    pipeline_activities[1][\"typeProperties\"] = {\"notebookId\": nb_200_id, \"workspaceId\": workspace_id}\n","    pipeline_activities[2][\"typeProperties\"] = {\"notebookId\": nb_300_id, \"workspaceId\": workspace_id}\n","\n","    for part in pipeline_definition[\"definition\"][\"parts\"]:\n","        part[\"payload\"] = base64.b64encode(json.dumps(part[\"payload\"]).encode(\"utf-8\")).decode(\"utf-8\")\n","    \n","    body = {\n","        \"definition\": pipeline_definition[\"definition\"]\n","    }\n","\n","    if existing_pipeline:\n","        print(f\"⚠️ Pipeline is already deployed under name {pipeline_name}, updating...\")\n","        data_pipeline_id = existing_pipeline[0]\n","        url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/dataPipelines/{data_pipeline_id}/updateDefinition\"\n","    else:\n","        print(f\"✅ Deploying pipeline: {pipeline_name}\")\n","        url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/dataPipelines\"\n","        body[\"displayName\"] = pipeline_name\n","        body[\"folderId\"] = folder_id\n","\n","    response = requests.post(url, headers=headers, json=body)\n","    response.raise_for_status()\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"022ce4fe-308c-45ad-9ac0-31688d15ad38"},{"cell_type":"code","source":["fabric_items = get_existing_items_in_folder(workspace_id, folder_id)\n","lakehouse_id = deploy_lakehouse(lakehouse_name, fabric_items, folder_id)\n","\n","gh_notebooks = [item for item in gh_items if item[\"name\"].endswith(\".ipynb\")]\n","for gh_notebook in gh_notebooks:\n","    if gh_notebook[\"name\"].endswith(\".ipynb\"):\n","        response = requests.get(gh_notebook[\"download_url\"])\n","        response.raise_for_status()\n","        notebook_content = response.content\n","        notebook_name = Path(gh_notebook[\"name\"]).stem\n","        deploy_notebook(notebook_name, notebook_content, fabric_items, folder_id=folder_id)\n","\n","print(f\"⌛ Waiting 30 seconds to allow all notebooks to be deployed\")\n","time.sleep(30)\n","\n","fabric_items = get_existing_items_in_folder(workspace_id, folder_id)\n","gh_pipeline = next(item for item in gh_items if item[\"name\"].startswith(\"PL_\"))\n","deploy_pipeline(gh_pipeline, fabric_items, folder_id=folder_id)\n","\n","for item in fabric_items:\n","    if item[\"type\"] == \"Notebook\":\n","        notebookutils.notebook.updateDefinition(\n","            name=item[\"displayName\"],\n","            workspaceId=workspace_id,\n","            defaultLakehouse=lakehouse_name, \n","            defaultLakehouseWorkspace=workspace_id, \n","        )\n","        print(f\"✅ Updated notebook {item['displayName']} to default lakehouse to {lakehouse_name}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"2cc36619-b74a-482d-8632-377ce58184c0"},{"cell_type":"markdown","source":["## Launch a first data pipeline run performing a full load"],"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"nteract":{"transient":{"deleting":false}}},"id":"e7a5e1e2-0e4e-47fc-8221-6766e25f1cb6"},{"cell_type":"code","source":["fabric_items = get_existing_items_in_folder(workspace_id, folder_id)\n","pipeline_id = next(item[\"id\"] for item in fabric_items if item[\"type\"] == \"DataPipeline\")  \n","url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/items/{pipeline_id}/jobs/instances?jobType=Pipeline\"\n","response = requests.post(url, headers=headers)  \n","response.raise_for_status()  "],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"bc6a0867-5dac-45f3-9429-3083cc1e7a46"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"jupyter","jupyter_kernel_name":"python3.11"},"kernelspec":{"display_name":"Jupyter","language":null,"name":"jupyter"},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"microsoft":{"language":"python","language_group":"jupyter_python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":null}},"nbformat":4,"nbformat_minor":5}
